import asyncio
import json
import logging
import os

from typing import Dict
from typing import List

import evaluate
import pandas as pd

from dotenv import load_dotenv

from apo import ChatGPT
from apo import MessageTemplate


logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)

load_dotenv()


class APO(ChatGPT):
    """APO class for generating synthetic gradients and editing prompts using synthetic gradient descent."""

    def __init__(self, asynchronous: bool = True, concurrency: int = 10, **kwargs) -> None:
        super().__init__(asynchronous=asynchronous, concurrency=concurrency, **kwargs)

    def build_path(self, filename: str) -> str:
        """Build the path to the prompt file to guarantee that it is always found"""
        path = os.path.join(os.path.dirname(__file__), filename)
        return path

    async def generate_gradients(self, prompt: str, error_str: str, num_feedbacks: int = 3, **openai_kwargs) -> str:
        """
        Asynchronously generate synthetic gradients for a given language prompt based on a specified error string.

        These gradients are textual feedback that point out the flaws or errors in the original prompt.

        Parameters
        ----------
        prompt : str
            The initial natural language prompt that is under optimization.
            This is the string for which the function will generate synthetic gradients to improve it.

        error_str : str
            A string that encapsulates the errors or issues with the predictions generated by the language model when
            using the initial prompt. This string is used by the language model to identify weaknesses in the original prompt.

        num_feedbacks : int, optional
            The number of synthetic gradients to generate. These are essentially pieces of feedback pointing out the issues
            or possible improvements for the original prompt. Default value is 3.

        **openai_kwargs : dict
            Additional keyword arguments that can be passed to the OpenAI API call.
            This allows for customization like setting the engine, max tokens, temperature, etc.

        """
        generate_gradients = MessageTemplate.load(
            self.build_path("./prompts/gradient_descent/generate_gradients/user.json")
        )
        generate_gradients.format_message(prompt=prompt, error_string=error_str, num_feedbacks=num_feedbacks)
        messages = [generate_gradients.to_prompt()]
        response = await self._agenerate(messages=messages, **openai_kwargs)
        gradients = response.content
        gradients_as_list = [reason["reason"] for reason in json.loads(gradients)["reasons"]]
        return "\n".join(gradients_as_list)

    async def edit_prompt_with_gradients(
        self, prompt: str, error_str: str, gradients: str, steps_per_gradient: int = 3, **openai_kwargs
    ) -> str:
        """
        Asynchronously edit and optimize a given natural language prompt based on provided synthetic gradients.

        This function uses OpenAI's API to iteratively improve the promptbased on the weaknesses identified by
        the synthetic gradients.

        Parameters
        ----------
        prompt : str
            The initial natural language prompt that needs to be optimized.
            This is the prompt for which the function will apply edits to improve its effectiveness.

        error_str : str
            A string that encapsulates the errors or shortcomings in the predictions generated by the language model
            when using the initial prompt. This string is used to provide context for the editing process.

        gradients : str
            A string containing synthetic gradients for the prompt. These gradients are textual feedback that point out
            flaws or potential areas for improvement in the original prompt.

        steps_per_gradient : int, optional
            The number of editing steps to take for each synthetic gradient. This controls how aggressively the prompt
            is edited. Default value is 3.

        **openai_kwargs : dict
            Additional keyword arguments that can be passed to the OpenAI API call.
            These could be settings like engine type, max tokens, temperature, etc.
        """

        edit_prompt = MessageTemplate.load("src/apo/prompts/gradient_descent/edit_prompt_w_gradient/user.json")
        edit_prompt.format_message(
            prompt=prompt, error_string=error_str, gradients=gradients, steps_per_gradient=steps_per_gradient
        )
        messages = [edit_prompt.to_prompt()]
        response = await self._agenerate(messages=messages, **openai_kwargs)
        return response.content

    async def run_prompt(self, prompt: MessageTemplate, prompt_args: Dict, **openai_kwargs) -> str:
        """Asynchronously evaluate a MessageTemplate prompt and return the output"""
        async with self.semaphore:
            prompt.format_message(**prompt_args)
            messages = [prompt.to_prompt()]
            response = await self._agenerate(messages=messages, **openai_kwargs)
            return response.content

    def compute_accuracy(self, predictions: pd.Series, labels: pd.Series) -> float:
        """Compute accuracy of predictions"""
        return (predictions == labels).sum() / len(predictions)

    def evaluate_predictions(self, predictions: List, labels: List, metric: str) -> float:
        """
        Evaluate the model's predictions using various metrics.

        Parameters:
        -----------
        predictions : pd.Series
            Series object containing the model's predictions.

        labels : pd.Series
            Series object containing the ground-truth labels.

        metric : str
            The type of metric to use for evaluation. Supported types are "accuracy", "f1", "recall", "rouge", and "bleu".

        Returns:
        --------
        result : float or dict
            The calculated metric value. For some metrics like 'rouge', a dictionary of values will be returned.
        """

        # Map metric names to metric functions
        metric_fn = evaluate.load(metric)

        # Compute the metric
        if metric in ["accuracy", "f1", "recall"]:
            result = metric_fn.compute(predictions=predictions, references=labels)
        elif metric == "rouge":
            result = metric_fn.compute(
                predictions=predictions, references=labels, rouge_types=["rouge1", "rouge2", "rougeL"]
            )
        elif metric == "bleu":
            result = metric_fn.compute(predictions=[predictions], references=[[labels]])
        else:
            raise ValueError(f"Unsupported metric: {metric}")
            return None

        return result

    async def evaluate_prompt(
        self,
        prompt: MessageTemplate,
        data: pd.DataFrame,
        input_cols: List,
        label_col: str = "label",
        metric: str = "accuracy",
        label_mapping: Dict = None,
        **openai_kwargs,
    ) -> float:
        """
        Evaluate a MessageTemplate prompt using a given dataset.

        Parameters:
        -----------
        prompt : MessageTemplate
            The MessageTemplate object to evaluate.

        data : pd.DataFrame
            The dataset to use for evaluation.

        input_cols : List
            The list of input columns to use for evaluation, only keep the columns that are required for the prompt.

        label_col : str
            The name of the column containing the ground-truth labels.
        """

        inputs = data[input_cols].to_dict("records")

        tasks = [self.run_prompt(prompt, prompt_input, **openai_kwargs) for prompt_input in inputs]

        results = await asyncio.gather(*tasks)

        if label_mapping is not None:
            results = [label_mapping[result] for result in results]

        return self.evaluate_predictions(results, data[label_col].tolist(), metric)
